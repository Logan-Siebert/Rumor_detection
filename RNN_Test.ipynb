{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import extract_dataset as dat\n",
    "import time_series_const as time\n",
    "import post_text_preprocess as pro\n",
    "import tfidf as tfidf\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "###################################################################################\n",
    "#                                                                                 #\n",
    "#       DATASET EXTRACTION                                                        #\n",
    "#                                                                                 #\n",
    "###################################################################################\n",
    "# 4664 labeled events \n",
    "# For the labels, the value is 1 if the event is a rumor, and is 0 otherwise. \n",
    "# The content of all the posts in are in json format (timestamp and text)\n",
    "# where each file is named event_id.json, corresponding to individual event\n",
    "\n",
    "n_ev = 4664 # number of evenements\n",
    "#event_ids = np.zeros((n_ev,1),dtype=int) #events ids\n",
    "#labels = np.zeros((n_ev,1),dtype=int) # labels 0 or 1\n",
    "event_ids = np.zeros((n_ev,1)) #events ids\n",
    "labels = np.zeros((n_ev,1),dtype=int) # labels 0 or 1\n",
    "\n",
    "# Extract labels and corresponding event ids\n",
    "event_related_posts = dat.extract_dataset(event_ids,labels,\"Weibo.txt\",n_ev)\n",
    "\n",
    "\n",
    "#Split the data into training and testing sets 80/20\n",
    "N_test = int(0.2 * event_related_posts.shape[0])\n",
    "\n",
    "#event_related_posts_train = event_related_posts[N_test:event_related_posts.shape[0],:]\n",
    "labels_train= labels[N_test:labels.shape[0],:]\n",
    "event_ids_train = event_ids[N_test:event_ids.shape[0],:]\n",
    "\n",
    "#event_related_posts_test = event_related_posts[0:N_test,:]\n",
    "labels_test= labels[0:N_test,:]\n",
    "event_ids_test = event_ids[0:N_test,:]\n",
    "\n",
    "\n",
    "    \n",
    "rnn_data_train=[] #time series tfidf RRN input data for each event\n",
    "rnn_data_test=[]\n",
    "\n",
    "## load event time series txt files (to save time)\n",
    "with open(\"training_event_time_series_tfidf_N30.txt\", \"rb\") as fp:   # Unpickling\n",
    "          rnn_data_train=(pickle.load(fp))        \n",
    "with open(\"test_event_time_series_tfidf_N30.txt\", \"rb\") as fp:   # Unpickling\n",
    "          rnn_data_test=(pickle.load(fp))\n",
    "          \n",
    "          \n",
    "# Keep only the K-most important score per interval\n",
    "# pad the number of intervals (each event need same number of intervals)\n",
    "# put each event in a numpy array\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3732, 2)\n",
      "(932, 2)\n"
     ]
    }
   ],
   "source": [
    "#Reshuffling the data to get a more even distribution of rumor and non-rumor event across training and test data\n",
    "\n",
    "import random\n",
    "reshuffled_labels = labels\n",
    "reshuffled_data = rnn_data_train.copy()\n",
    "reshuffled_data.extend(rnn_data_test)\n",
    "temp = list(zip(reshuffled_labels, reshuffled_data))\n",
    "\n",
    "random.shuffle(temp)\n",
    "reshuffled_labels, reshuffled_data = zip(*temp)\n",
    "reshuffled_labels = np.array(reshuffled_labels)\n",
    "\n",
    "labels_train= reshuffled_labels[N_test:reshuffled_labels.shape[0],:]\n",
    "labels_test= reshuffled_labels[0:N_test,:]\n",
    "\n",
    "rnn_data_train = [sublist for sublist in reshuffled_data[N_test:]]\n",
    "rnn_data_test = [sublist for sublist in reshuffled_data[0:N_test]]\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(labels_train)\n",
    "labels_train = enc.transform(labels_train).toarray()\n",
    "enc.fit(labels_test)\n",
    "labels_test = enc.transform(labels_test).toarray()\n",
    "\n",
    "\n",
    "print(labels_train.shape)\n",
    "print(labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest K value in all intervals = 389739\n",
      "Largest # intervals in a single event = 58\n"
     ]
    }
   ],
   "source": [
    "#Just checking what the max number of tf.idf values (maxK) inside any interval in the data is\n",
    "maxK = 0\n",
    "for event in rnn_data_train:\n",
    "    maxK = max(len(max(event,key=len)),maxK)\n",
    "print(\"Largest K value in all intervals = \" + str(maxK))\n",
    "maxNrIntervals = max( len(max(rnn_data_train,key=len)), len(max(rnn_data_test,key=len)))\n",
    "print(\"Largest # intervals in a single event = \" + str(maxNrIntervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 100 #the number of tf.idf values sorted in descending order we will keep for each interval\n",
    "# maxNrIntervals = 30 #equivalent to N value \n",
    "\n",
    "# print(maxNrIntervals)\n",
    "# new_rnn_train = []\n",
    "# new_rnn_test = []\n",
    "# #Processing Training Data\n",
    "# for event in rnn_data_train:\n",
    "#     new_event = []\n",
    "#     for interval in event: \n",
    "#         kInterval = sorted(interval, reverse=True)[:k]\n",
    "#         kInterval.extend([0]*(k-len(kInterval))) #append the interval with zeros until it has a length of k\n",
    "#         new_event.append(kInterval)\n",
    "#         if len(new_event) == maxNrIntervals: break\n",
    "#     while len(new_event) < maxNrIntervals:\n",
    "#         new_event.append([0]*k) #append the event with intervals of zeros until it has a length of maxNrIntervals\n",
    "#     new_rnn_train.append(new_event)\n",
    "\n",
    "# #Processing Test Data\n",
    "# for event in rnn_data_test:\n",
    "#     new_event = []\n",
    "#     for interval in event: \n",
    "#         kInterval = sorted(interval, reverse=True)[:k]\n",
    "#         kInterval.extend([0]*(k-len(kInterval))) #append the interval with zeros until it has a length of k\n",
    "#         new_event.append(kInterval)\n",
    "#         if len(new_event) == maxNrIntervals: break\n",
    "#     while len(new_event) < maxNrIntervals:\n",
    "#         new_event.append([0]*k) #append the event with intervals of zeros until it has a length of maxNrIntervals\n",
    "#     new_rnn_test.append(new_event)\n",
    "        \n",
    "# new_rnn_train = np.array(new_rnn_train) #convert the standard python lists to numpy arrays\n",
    "# norm = np.linalg.norm(new_rnn_train)\n",
    "# new_rnn_train = new_rnn_train/norm\n",
    "# new_rnn_test = np.array(new_rnn_test)\n",
    "# new_rnn_test = new_rnn_test/norm\n",
    "# print(new_rnn_train.shape)\n",
    "# print(new_rnn_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "Start for train\n",
      "Start for test  \n",
      "(3732, 30, 5000)\n",
      "(932, 30, 5000)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "k = 5000 #the number of tf.idf values sorted in descending order we will keep for each interval\n",
    "maxNrIntervals = 30 #equivalent to N value \n",
    "\n",
    "print(maxNrIntervals)\n",
    "new_rnn_train = []\n",
    "new_rnn_test = []\n",
    "#Processing Training Data\n",
    "print(\"Start for train\")\n",
    "for idx, event in enumerate(rnn_data_train):\n",
    "    sys.stdout.write(\"Progress: %d%%   \\r\" % (idx/len(rnn_data_train)*100) )\n",
    "    sys.stdout.flush()\n",
    "    new_event = []\n",
    "    for interval in event: \n",
    "        highestVals = (-np.array(interval)).argsort()[:k]\n",
    "        kInterval = []\n",
    "        for i, val in enumerate(interval):\n",
    "            if (i in highestVals):\n",
    "               kInterval.append(val) \n",
    "        kInterval.extend([0]*(k-len(kInterval))) #append the interval with zeros until it has a length of k \n",
    "        new_event.append(kInterval)\n",
    "        if len(new_event) == maxNrIntervals: break\n",
    "    while len(new_event) < maxNrIntervals:\n",
    "        new_event.append([0]*k) #append the event with intervals of zeros until it has a length of maxNrIntervals\n",
    "    new_rnn_train.append(new_event)\n",
    "\n",
    "#Processing Test Data\n",
    "print(\"Start for test\")\n",
    "for idx, event in enumerate(rnn_data_test):\n",
    "    sys.stdout.write(\"Progress: %d%%   \\r\" % (idx/len(rnn_data_test)*100) )\n",
    "    sys.stdout.flush()\n",
    "    new_event = []\n",
    "    for interval in event: \n",
    "        highestVals = (-np.array(interval)).argsort()[:k]\n",
    "        kInterval = []\n",
    "        for i, val in enumerate(interval):\n",
    "            if (i in highestVals):\n",
    "               kInterval.append(val) \n",
    "        kInterval.extend([0]*(k-len(kInterval))) #append the interval with zeros until it has a length of k\n",
    "        new_event.append(kInterval)\n",
    "        if len(new_event) == maxNrIntervals: break\n",
    "    while len(new_event) < maxNrIntervals:\n",
    "        new_event.append([0]*k) #append the event with intervals of zeros until it has a length of maxNrIntervals\n",
    "    new_rnn_test.append(new_event)\n",
    "        \n",
    "new_rnn_train = np.array(new_rnn_train) #convert the standard python lists to numpy arrays\n",
    "norm = np.linalg.norm(new_rnn_train)\n",
    "new_rnn_train = new_rnn_train/norm\n",
    "new_rnn_test = np.array(new_rnn_test)\n",
    "new_rnn_test = new_rnn_test/norm\n",
    "print(new_rnn_train.shape)\n",
    "print(new_rnn_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.17646808e-03, 5.83095667e-04, 5.39647913e-04, ...,\n",
       "        1.94365222e-04, 3.02915842e-04, 3.02915842e-04],\n",
       "       [1.55889018e-04, 1.38132261e-04, 1.38132261e-04, ...,\n",
       "        1.81870521e-04, 3.22308608e-04, 3.22308608e-04],\n",
       "       [1.65028287e-03, 1.65028287e-03, 4.00019122e-04, ...,\n",
       "        2.75047144e-04, 2.75047144e-04, 2.00009561e-04],\n",
       "       ...,\n",
       "       [3.75187919e-05, 3.75187919e-05, 3.75187919e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rnn_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.device('/gpu:1') #My best gpu is gpu:1, change to gpu:0 if you only have 1 gpu\n",
    "CUDA_VISIBLE_DEVICES=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3732 samples, validate on 932 samples\n",
      "Epoch 1/10\n",
      "3732/3732 [==============================] - 4s 1ms/sample - loss: 0.8747 - accuracy: 0.5027 - val_loss: 0.6951 - val_accuracy: 0.4936\n",
      "Epoch 2/10\n",
      "3732/3732 [==============================] - 2s 606us/sample - loss: 0.6936 - accuracy: 0.5056 - val_loss: 0.6946 - val_accuracy: 0.5064\n",
      "Epoch 3/10\n",
      "3732/3732 [==============================] - 2s 598us/sample - loss: 0.6939 - accuracy: 0.5083 - val_loss: 0.6931 - val_accuracy: 0.5064\n",
      "Epoch 4/10\n",
      "3732/3732 [==============================] - 2s 608us/sample - loss: 0.6937 - accuracy: 0.4992 - val_loss: 0.6934 - val_accuracy: 0.5064\n",
      "Epoch 5/10\n",
      "3732/3732 [==============================] - 2s 605us/sample - loss: 0.6938 - accuracy: 0.4922 - val_loss: 0.6933 - val_accuracy: 0.5064\n",
      "Epoch 6/10\n",
      "3732/3732 [==============================] - 2s 604us/sample - loss: 0.6936 - accuracy: 0.5024 - val_loss: 0.6935 - val_accuracy: 0.5064\n",
      "Epoch 7/10\n",
      "3732/3732 [==============================] - 2s 610us/sample - loss: 0.6934 - accuracy: 0.5067 - val_loss: 0.6946 - val_accuracy: 0.5064\n",
      "Epoch 8/10\n",
      "3732/3732 [==============================] - 2s 594us/sample - loss: 0.6937 - accuracy: 0.4965 - val_loss: 0.6932 - val_accuracy: 0.5064\n",
      "Epoch 9/10\n",
      "3732/3732 [==============================] - 2s 618us/sample - loss: 0.6936 - accuracy: 0.4981 - val_loss: 0.6933 - val_accuracy: 0.5064\n",
      "Epoch 10/10\n",
      "3732/3732 [==============================] - 2s 618us/sample - loss: 0.6935 - accuracy: 0.4842 - val_loss: 0.6931 - val_accuracy: 0.5064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x179003b4c88>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(maxNrIntervals, activation='tanh',use_bias=True, kernel_initializer='uniform',\n",
    "                   recurrent_initializer='orthogonal', kernel_regularizer=tf.keras.regularizers.l2(l=1),\n",
    "                    bias_initializer='zeros',dropout=0.0, recurrent_dropout=0.0,\n",
    "                   return_sequences=False))\n",
    "model.add(Dense(2,activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(l=1)))\n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(lr=0.1, initial_accumulator_value=0.9, epsilon=1e-07)\n",
    "#opt = tf.keras.optimizers.Adam(lr=1e-3, decay=1e-5)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    #this should be mse between the probability distributions of the prediction and ground truth + L2-regularization penalty\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.fit(new_rnn_train,\n",
    "          labels_train,\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          validation_data=(new_rnn_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
