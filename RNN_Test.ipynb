{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import extract_dataset as dat\n",
    "import time_series_const as time\n",
    "import post_text_preprocess as pro\n",
    "import tfidf as tfidf\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "###################################################################################\n",
    "#                                                                                 #\n",
    "#       DATASET EXTRACTION                                                        #\n",
    "#                                                                                 #\n",
    "###################################################################################\n",
    "# 4664 labeled events \n",
    "# For the labels, the value is 1 if the event is a rumor, and is 0 otherwise. \n",
    "# The content of all the posts in are in json format (timestamp and text)\n",
    "# where each file is named event_id.json, corresponding to individual event\n",
    "\n",
    "n_ev = 4664 # number of evenements\n",
    "#event_ids = np.zeros((n_ev,1),dtype=int) #events ids\n",
    "#labels = np.zeros((n_ev,1),dtype=int) # labels 0 or 1\n",
    "event_ids = np.zeros((n_ev,1)) #events ids\n",
    "labels = np.zeros((n_ev,1)) # labels 0 or 1\n",
    "\n",
    "# Extract labels and corresponding event ids\n",
    "event_related_posts = dat.extract_dataset(event_ids,labels,\"Weibo.txt\",n_ev)\n",
    "\n",
    "# i=0 \n",
    "# j=0\n",
    "# for event_id in event_ids:\n",
    "#     filename = 'Weibo/%d.json' %event_id #event file with corresponding posts\n",
    "#     with open(filename, 'r') as myfile:\n",
    "#         data=myfile.read()\n",
    "#     myfile.close()\n",
    "#     posts = json.loads(data) #event related posts\n",
    "#     for post in posts:\n",
    "#         event_related_posts[i,j]=post['t'] #timestamp of post\n",
    "#         j=j+1\n",
    "#     event_related_posts[i]= np.sort(event_related_posts[i])\n",
    "#     i=i+1\n",
    "#     j=0\n",
    "\n",
    "#Split the data into training and testing sets 80/20\n",
    "N_test = int(0.2 * event_related_posts.shape[0])\n",
    "\n",
    "#event_related_posts_train = event_related_posts[N_test:event_related_posts.shape[0],:]\n",
    "labels_train= labels[N_test:labels.shape[0],:]\n",
    "event_ids_train = event_ids[N_test:event_ids.shape[0],:]\n",
    "\n",
    "#event_related_posts_test = event_related_posts[0:N_test,:]\n",
    "labels_test= labels[0:N_test,:]\n",
    "event_ids_test = event_ids[0:N_test,:]\n",
    "\n",
    "### load pre-saved arrays for time saving\n",
    "#event_related_posts_train =np.load('event_related_posts_train_array.npy')\n",
    "#event_related_posts_test =np.load('event_related_posts_test_array.npy')\n",
    "\n",
    "###################################################################################\n",
    "#                                                                                 #\n",
    "#       EVENTS TIME SERIES CONSTRUCTION                                           #\n",
    "#                                                                                 #\n",
    "###################################################################################\n",
    "N=20; # RNN reference length N\n",
    "#time_series_train = time.events_time_series(event_related_posts_train,N)\n",
    "#time_series_test = time.events_time_series(event_related_posts_test,N)\n",
    "  \n",
    "###################################################################################\n",
    "#                                                                                 #\n",
    "#       TEXTUAL FEATURES EXTRACTION FOR THE TIME SERIES EVENTS                    #\n",
    "#                                                                                 #\n",
    "################################################################################### \n",
    "# Each json file (name=event_id (=1st post ID)) contains posts texts (chinese)\n",
    "# TF.IDF on each intervals => inputs to RNN\n",
    "# read time series file and .json corresponding post texts  \n",
    "    \n",
    "rnn_data_train=[] #time series tfidf RRN input data for each event\n",
    "rnn_data_test=[]\n",
    "\n",
    "#tfidf event: List of event intervals. Each element is dict\n",
    "#word:tfidf value. Intervals comprise several words-tfidf score pairs\n",
    "#according to the time series construction of the posts\n",
    "#we only keep tfidf scores\n",
    "\n",
    "# for j in range(event_related_posts_train.shape[0]):\n",
    "#     print(\"training event: %d/3732\" %(j+1))\n",
    "#     filename = 'Weibo/%d.json' %event_ids_train[j] #training event\n",
    "#     with open(filename, 'r') as myfile:\n",
    "#             data=myfile.read()\n",
    "#     myfile.close()\n",
    "#     posts = json.loads(data) #training event related posts\n",
    "  \n",
    "#     tfidf_event=[]\n",
    "#     event = time_series_train[j]\n",
    "#     for i in range(event.shape[0]): #TFIDF for each interval\n",
    "#         temp = event[i]\n",
    "#         post_text = pro.post_text_preprocess(temp,posts)\n",
    "#         #TF-IDF\n",
    "#         TF_IDF = tfidf.tfidf(post_text)\n",
    "#         if(TF_IDF!=[]):\n",
    "#             tfidf_event.append(TF_IDF)\n",
    "#     if(tfidf_event!=[]):\n",
    "#         rnn_data_train.append(tfidf_event)\n",
    "#     posts=[]\n",
    "\n",
    "# for j in range(event_related_posts_test.shape[0]):\n",
    "#     print(\"test event: %d/932\" %(j+1))\n",
    "#     filename = 'Weibo/%d.json' %event_ids_test[j] #training event\n",
    "#     with open(filename, 'r') as myfile:\n",
    "#             data=myfile.read()\n",
    "#     myfile.close()\n",
    "#     posts = json.loads(data) #training event related posts\n",
    "  \n",
    "#     tfidf_event=[]\n",
    "#     event = time_series_test[j]\n",
    "#     for i in range(event.shape[0]): #TFIDF for each interval\n",
    "#         temp = event[i]\n",
    "#         post_text = pro.post_text_preprocess(temp,posts)\n",
    "#         #TF-IDF\n",
    "#         TF_IDF = tfidf.tfidf(post_text)\n",
    "#         if(TF_IDF!=[]):\n",
    "#             tfidf_event.append(TF_IDF)\n",
    "#     if(tfidf_event!=[]):\n",
    "#         rnn_data_test.append(tfidf_event)\n",
    "#     posts=[]\n",
    "\n",
    "# with open(\"training_event_time_series_tfidf.txt\", \"wb\") as fp:\n",
    "#       pickle.dump(rnn_data_train, fp)    \n",
    "# with open(\"test_event_time_series_tfidf.txt\", \"wb\") as fp:\n",
    "#       pickle.dump(rnn_data_test, fp)\n",
    "\n",
    "\n",
    "## load event time series txt files (to save time)\n",
    "with open(\"training_event_time_series_tfidf.txt\", \"rb\") as fp:   # Unpickling\n",
    "          rnn_data_train=(pickle.load(fp))        \n",
    "with open(\"test_event_time_series_tfidf.txt\", \"rb\") as fp:   # Unpickling\n",
    "          rnn_data_test=(pickle.load(fp))\n",
    "          \n",
    "          \n",
    "# Keep only the K-most important score per interval\n",
    "# pad the number of intervals (each event need same number of intervals)\n",
    "# put each event in a numpy array\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest K value in all intervals = 391445\n",
      "Largest # intervals in a single event = 39\n"
     ]
    }
   ],
   "source": [
    "#Just checking what the max number of tf.idf values (maxK) inside any interval in the data is\n",
    "maxK = 0\n",
    "for event in rnn_data_train:\n",
    "    maxK = max(len(max(event,key=len)),maxK)\n",
    "print(\"Largest K value in all intervals = \" + str(maxK))\n",
    "maxNrIntervals = max( len(max(rnn_data_train,key=len)), len(max(rnn_data_test,key=len)))\n",
    "print(\"Largest # intervals in a single event = \" + str(maxNrIntervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "(3732, 20, 5000)\n",
      "(932, 20, 5000)\n"
     ]
    }
   ],
   "source": [
    "k = 5000 #the number of tf.idf values sorted in descending order we will keep for each interval\n",
    "maxNrIntervals = 20 #equivalent to N value \n",
    "\n",
    "print(maxNrIntervals)\n",
    "new_rnn_train = []\n",
    "new_rnn_test = []\n",
    "#Processing Training Data\n",
    "for event in rnn_data_train:\n",
    "    new_event = []\n",
    "    for interval in event: \n",
    "        kInterval = sorted(interval, reverse=True)[:k]\n",
    "        kInterval.extend([0]*(k-len(kInterval))) #append the interval with zeros until it has a length of k\n",
    "        new_event.append(kInterval)\n",
    "        if len(new_event) == maxNrIntervals: break\n",
    "    while len(new_event) < maxNrIntervals:\n",
    "        new_event.append([0]*k) #append the event with intervals of zeros until it has a length of maxNrIntervals\n",
    "    new_rnn_train.append(new_event)\n",
    "\n",
    "#Processing Test Data\n",
    "for event in rnn_data_test:\n",
    "    new_event = []\n",
    "    for interval in event: \n",
    "        kInterval = sorted(interval, reverse=True)[:k]\n",
    "        kInterval.extend([0]*(k-len(kInterval))) #append the interval with zeros until it has a length of k\n",
    "        new_event.append(kInterval)\n",
    "        if len(new_event) == maxNrIntervals: break\n",
    "    while len(new_event) < maxNrIntervals:\n",
    "        new_event.append([0]*k) #append the event with intervals of zeros until it has a length of maxNrIntervals\n",
    "    new_rnn_test.append(new_event)\n",
    "        \n",
    "new_rnn_train = np.array(new_rnn_train) #convert the standard python lists to numpy arrays\n",
    "new_rnn_test = np.array(new_rnn_test)\n",
    "print(new_rnn_train.shape)\n",
    "print(new_rnn_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.device('/gpu:1') #My best gpu is gpu:1, change to gpu:0 if you only have 1 gpu\n",
    "CUDA_VISIBLE_DEVICES=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3732 samples, validate on 932 samples\n",
      "Epoch 1/10\n",
      "3732/3732 [==============================] - 6s 1ms/sample - loss: 0.6116 - accuracy: 0.6702 - val_loss: 0.7832 - val_accuracy: 0.5343\n",
      "Epoch 2/10\n",
      "3732/3732 [==============================] - 2s 561us/sample - loss: 0.5848 - accuracy: 0.7093 - val_loss: 1.0540 - val_accuracy: 0.3906\n",
      "Epoch 3/10\n",
      "3732/3732 [==============================] - 2s 556us/sample - loss: 0.5605 - accuracy: 0.7278 - val_loss: 1.1118 - val_accuracy: 0.2972\n",
      "Epoch 4/10\n",
      "3732/3732 [==============================] - 2s 570us/sample - loss: 0.5400 - accuracy: 0.7438 - val_loss: 1.4082 - val_accuracy: 0.2082\n",
      "Epoch 5/10\n",
      "3732/3732 [==============================] - 2s 569us/sample - loss: 0.5225 - accuracy: 0.7532 - val_loss: 0.5122 - val_accuracy: 0.7296\n",
      "Epoch 6/10\n",
      "3732/3732 [==============================] - 2s 571us/sample - loss: 0.5101 - accuracy: 0.7591 - val_loss: 0.9444 - val_accuracy: 0.4903\n",
      "Epoch 7/10\n",
      "3732/3732 [==============================] - 2s 567us/sample - loss: 0.4931 - accuracy: 0.7741 - val_loss: 1.3028 - val_accuracy: 0.3594\n",
      "Epoch 8/10\n",
      "3732/3732 [==============================] - 2s 573us/sample - loss: 0.4799 - accuracy: 0.7797 - val_loss: 0.7073 - val_accuracy: 0.6245\n",
      "Epoch 9/10\n",
      "3732/3732 [==============================] - 2s 559us/sample - loss: 0.4750 - accuracy: 0.7832 - val_loss: 0.9556 - val_accuracy: 0.4742\n",
      "Epoch 10/10\n",
      "3732/3732 [==============================] - 2s 574us/sample - loss: 0.4567 - accuracy: 0.7894 - val_loss: 0.9933 - val_accuracy: 0.4549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x222ba46eb08>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#model.add(Embedding(input_dim=k, output_dim=100, input_length=maxNrIntervals)) #no idea yet how to get this working for our input\n",
    "#model.add(LSTM(maxNrIntervals, input_shape=(new_rnn_train.shape[1:]), activation='relu', return_sequences=True))\n",
    "model.add(LSTM(maxNrIntervals, input_shape=(new_rnn_train.shape[1:])))\n",
    "#model.add(Dropout(0.2)) #is this really necessary?\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(lr=0.5, decay=1e-6) #paper uses Adagrad instead of Adam with LR of 0.5 (no mention of decay rate)\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    #this should be mse between the probability distributions of the prediction and ground truth + L2-regularization penalty\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.fit(new_rnn_train,\n",
    "          labels_train,\n",
    "          epochs=10,\n",
    "          validation_data=(new_rnn_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
